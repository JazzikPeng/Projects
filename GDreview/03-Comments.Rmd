```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.align = "center",fig.width = 3,fig.height = 3)
```

# Test

**Optimal step-size.** The step-size $h^k$ plays a key role in the performance of the gradient descent method. If $h^k$ is too small, more iterations will be required to achieve the $\epsilon$-accuracy ($|f(x^k)-f(x^*)|\leq \epsilon$). If $h^k$ is too large, the scheme fails to converge. We here present two theoretical optimal step-size selection rules.

* If $f(x)$ is gradient Lipschtiz continuous, then the optimal $h^k = \frac{1}{L}$.(This conclusion dose not require the convexity assumption.) 
* If $f(x)$ is gradient Lipschtiz continuous and strongly convex, then the optimal $h^k = \frac{2}{L+\ell}$.

## Why linear rate of convergence? 

Before we answer this question, we first list the definition of a contraction mapping and the Banach fixed-point theorem.

```{definition, name="Contraction Mapping"}
Let $(X, d)$ be a metric space. Then a map $T : X \rightarrow X$ is called a contraction mapping on $X$ if there exists $q \in [0, 1)$ such that

$$ d(T(x),T(y))\leq qd(x,y) \forall x, y \in X.$$

```

```{theorem, name="Banach Fixed Point Theorem"}
Let $(X, d)$ be a non-empty complete metric space with a contraction mapping $T : X \rightarrow X$. Then $T$ admits a unique fixed-point $x^*$ in $X$ (i.e. $T(x^*) = x^*$). Furthermore, $x^*$ can be found as follows: start with an arbitrary element $x_0$ in $X$ and define a sequence $\{x_n\}$ by $x_n = T(x_{nâˆ’1})$, then $x_n \rightarrow x^*$. Moreover,
$$ d(x^{*},x_{n+1})\leq qd(x^{*},x_{n}) $$
```

Then we establish connections between contraction mapping and linear rate of convergence through the gradient decent method, which takes the form of 

$$x^{k+1} = x^k - \alpha\nabla f(x^k),$$
If we  define a mapping as $g(x) = x - \alpha\nabla f(x)$ and suppose $g(x)$ is contractive on $R^n$. Assume there exists a stationary point $x^*$, then starting from any point $x^0$, we derive

$$
\begin{align}
||x^{k+1} - x^*|| &= ||x^{k} - \alpha\nabla f(x^k) - x^*|| \\
                  &= || g(x^k) - g(x^*)|| \\
                  & \leq q||x^k - x^*|| \\
                  & \leq q^{k+1}|| x^0 - x^* || (\#eq:linear)
\end{align}
$$
\@ref(eq:linear) indicates that the rate of convergence is linear.

Finally, we assert that 

```{lemma}
If $f(x)$ is twice continuously differentiable, the $g(x)$ is a comtraction mapping if and only if $f(x)$ is strongly convex on $R^n$.
```

```{proof}
* Suppose $g(x)$ is contractivie, then we derive,
\begin{align}

\lim_{t\rightarrow 0^+} \frac{1}{t} || g(x + t\Delta x) - g(x) || 
&=  \lim_{t\rightarrow 0^+}  || \Delta x - \frac{\alpha}{t}[\nabla f(x + t\Delta x) - \nabla f(x)] || \\
&= || \Delta x - \alpha \nabla^2 f(x)\Delta x || \\
& \leq q||\Delta x|| (\#eq:convex)

\end{align}

\@ref(eq:convex) holds by the contractivity and it means that $||I_n - \alpha\nabla^2 f(x)||\leq q$. Therefore, we derive

$$ \frac{1-q}{\alpha}I_n  \leq \nabla^2 f(x) \leq \frac{1+q}{\alpha} I_n $$. Therefore $f(x)$ is strongly convex.

* Suppose $f(x)$ is strongly convex, go backward of the above proof, we assert $g(x)$ is contractive.
```
## Error bounds



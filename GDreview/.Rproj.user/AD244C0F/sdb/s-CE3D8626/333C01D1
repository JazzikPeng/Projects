{
    "collab_server" : "",
    "contents" : "\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE,fig.align = \"center\",fig.width = 10,fig.height = 8)\n```\n\n# Gradient Decent Methods\n\n| Problem Formulation| $\\min_{x\\in R^n} f(x)$ |\n| :---: | :--- |\n| Function class |  Continuously Differentiable and bouded below $R^n$|\n| Goal | Find local minimum $x^{*}$ |\n| Scheme | Choose $x^0\\in R^n$, and run $x^{k+1} = x^k - \\alpha^k\\nabla f(x^k)$ |\n\n## Problem with additional assumptions\n\nIn many literatures, problems of interest are naturally endowed with some basic assumptions so that beautiful analytical results can be derived, such as the convergence rate. One might ask that why these assumptioms are important and what are intuitions behind these assumtions? We try to answer these questions in this section.\n\n### Lipschtiz constant\n\nProblems of interest are naturally endowed with the *Lipschitz continous gradient* property, that is \n\n```{definition}\nWe can $f(x)$ is gradient Lipschitz continous if  \n$$||\\nabla f(x) - \\nabla f(y) || \\leq L || x - y|| \\forall x,y\\in R^n.$$\nHere $||.||$ means the Euclidean norm.\n```\n\n**Remark** Intuitively, the Lipschitz continous gradient property enforces $f(x)$ not to be “too sharp”. For example, the $f(x) = |x|$ dosen't enjoy this property.\n\n**Remark** If $f(x)$ is gradient Lipschitz continous, then it is upper bounded by a quadratic function $\\phi_1(x)$, that is\n\n$$ f(x) \\leq f(x_0) + \\nabla f(x_0)^T(x-x_0) + \\frac{L}{2} ||x-x_0||^2 := \\phi_1(x) \\quad \\forall x,x_0\\in R^n $$\nalso $f(x)$ is the upper bound of another quadratic function $\\phi_2(x)$, taking form of \n\n$$ \\phi_2(x) := f(x^0) + \\nabla f(x_0)^T(x-x_0) - \\frac{L}{2} ||x-x_0||^2. $$\nSee Figure\\@ref(fig:geomlip) for example.\n\n```{r geomlip, echo = F, fig.cap=\"An illustration of the gradient Lipschtiz continuous propety.\"}\nknitr::include_graphics(\"pics/geomlip.png\")\n```\n\nTo verify this remark, we can apply the following lemma.\n\n```{lemma, label=\"bound\"}\nIf $f(x)$ is continuously differentiable and gradient Lipschtiz continuous, then for any $x,y$ the following holds:\n\n$$ | f(x) - f(y) - (y-x)^T\\nabla f(x)| \\leq \\frac{L}{2} || y-x ||^2$$\n```\n\n```{corollary}\nFrom Lemma\\@ref(lem:bound), we assert that if  $f(x)$ is continuously differentiable and gradient Lipschtiz continuous then the Hessian of $f(x)$ is bounded by $LI_n$, where $I_n$ is $n\\times n$ unit matrix.\n\n```\n\n\nA easy way to check the *gradient Lipschitz continous* assumption is to apply the Lemma\\@ref(lem:L-inclusion)[@nesterov2013introductory]\n\n```{lemma, label=\"L-inclusion\"}\nIf $f(x)$ is twice differentiable and satisfies $||\\nabla f(x) - \\nabla f(y) || \\leq L || x - y||$ if and only if\n\n$$ ||\\nabla^2 f(x) || \\leq L, \\forall x\\in R^n. $$\n\nFor matrix case, $||A|| = \\sqrt{\\lambda_{max}(A^TA)},\\forall A\\in R^{n\\times n}.$\n```\n\n### Convexity\n\nIf $f(x)$ is convex, we enjoy the following nice property:\n\n* The local minimum we find is also the global minimum;\n* Using the First order optimality condition, we can dramatically simplify our problem. That is, we only have to construct a sequence $\\{x^k\\}$ that satifies $\\lim_{k\\rightarrow +\\infty}||\\nabla f(x^k)||=0$\n\n### Strong Convexity \nWe now only present a geometric interpretation of strong convexity and will go back to this point later when talking about the rate of convergence.\n\n```{definition}\nWe can $f(x)$ is strongly convex if\n$$ f(x) \\geq f(x_0) + \\nabla f(x_0)^T(x-x_0) + \\frac{\\ell}{2} ||x-x_0||^2  \\forall x,x_0\\in R^n $$\n\n```\n\nThis means that we can squeeze a parabola between the tangent plane at $x_0$ given by the gradient and the function itself. Thus, the strong convexity assumption enforce the $f(x)$ not to be too \"flat\".\n\n```{r geomstrong, echo = F, fig.cap=\"An illustration of the strongly convex propety.\"}\nknitr::include_graphics(\"pics/geomstrong.png\")\n```\n\n## Convex situations\n\nWithout additional assumptions, we can not guarantee convergence even to a local minimum and estimate the global performance of the proposed schemes. In general, the gradient method convergece only to a stationary point of $f(x)$. Therefore, from now on, we only focus on convex cases.\n\n### step-size selection rule and the rate of convergence\n\nThe step-size $\\alpha^k$ plays a key role in the convergence of the gradient descent method. If $\\alpha^k$ is too small, more iterations will be required to achieve the $\\epsilon$-accuracy ($|f(x^k)-f(x^*)|\\leq \\epsilon$). If $\\alpha^k$ is too large, the scheme fails to converge. We here present two theoretical optimal step-size selection rules.\n\n* If convex $f(x)$ is gradient Lipschtiz continuous, then the optimal $\\alpha^k = \\frac{1}{L}$.\n\n* If strongly convex $f(x)$ is gradient Lipschtiz continuous, then the optimal $\\alpha^k = \\frac{2}{L+\\ell}$.\n\nHere, *optimal* means \n\n$$\n\\alpha^k = \\mathop{\\text{argmin}}_{\\alpha} f(x^k - \\alpha\\nabla f(x^k)).\n$$\nIt has been proved that \n\n* For convex and gradient Lipschtiz continuous $f(x)$, with chosen step-size $\\alpha^k = \\frac{1}{L}$ , the rate of convergence is sub-linear. To be specific, \n\n  $$f(x_k) - f(x^*) \\leq \\frac{2L||x^0 - x^*||^2}{k+4}.$$\n  \n* For strongly convex and gradient Lipschtiz continuous $f(x)$, with chosen step-size $\\alpha^k = \\frac{2}{L + \\ell}$ , the rate of convergence is linear. To be specific,\n\n  $$f(x_k) - f(x^*) \\leq \\frac{L}{2}\\big(\\frac{L-\\ell}{L+\\ell}\\big)^{2k}||x^0 - x^*||^2.$$\nTechnically, the above results are mainly derived from two steps:\n\n*  Find how much you can improve in a single interation by establishing\n\n$$f(x^{k+1}) \\leq  f(x^k) - \\omega_1 ||\\nabla f(x_k)|| $$\n\n* Find how far at most we still need to go to recach the minimum by establishing\n\n$$f(x^k) - f(x^*) \\leq \\omega_2||\\nabla f(x_k)||$$\n\n### Why linear rate of convergence? \n\nBefore we answer this question, we first list the definition of a contraction mapping and the Banach fixed-point theorem.\n\n```{definition, name=\"Contraction Mapping\"}\nLet $(X, d)$ be a metric space. Then a map $T : X \\rightarrow X$ is called a contraction mapping on $X$ if there exists $q \\in [0, 1)$ such that\n\n$$ d(T(x),T(y))\\leq qd(x,y) \\forall x, y \\in X.$$\n\n```\n\n```{theorem, name=\"Banach Fixed Point Theorem\"}\nLet $(X, d)$ be a non-empty complete metric space with a contraction mapping $T : X \\rightarrow X$. Then $T$ admits a unique fixed-point $x^*$ in $X$ (i.e. $T(x^*) = x^*$). Furthermore, $x^*$ can be found as follows: start with an arbitrary element $x_0$ in $X$ and define a sequence $\\{x_n\\}$ by $x_n = T(x_{n−1})$, then $x_n \\rightarrow x^*$. Moreover,\n$$ d(x^{*},x_{n+1})\\leq qd(x^{*},x_{n}) $$\n```\n\nThen we establish connections between contraction mapping and linear rate of convergence through the gradient decent method, which takes the form of \n\n$$x^{k+1} = x^k - \\alpha\\nabla f(x^k),$$\nIf we  define a mapping as $g(x) = x - \\alpha\\nabla f(x)$ and suppose $g(x)$ is contractive on $R^n$. Assume there exists a stationary point $x^*$, then starting from any point $x^0$, we derive\n\n$$\n\\begin{align}\n||x^{k+1} - x^*|| &= ||x^{k} - \\alpha\\nabla f(x^k) - x^*|| \\\\\n                  &= || g(x^k) - g(x^*)|| \\\\\n                  & \\leq q||x^k - x^*|| \\\\\n                  & \\leq q^{k+1}|| x^0 - x^* || (\\#eq:linear)\n\\end{align}\n$$\n\\@ref(eq:linear) indicates that the rate of convergence is linear.\n\nFinally, we assert that \n\n```{lemma}\nIf $f(x)$ is twice continuously differentiable, the $g(x)$ is a contraction mapping if and only if $f(x)$ is strongly convex on $R^n$.\n```\n\n\n```{proof}\nSuppose $g(x)$ is contractivie, then we derive,\n\\begin{align}\n\n\\lim_{t\\rightarrow 0^+} \\frac{1}{t} || g(x + t\\Delta x) - g(x) || \n&=  \\lim_{t\\rightarrow 0^+}  || \\Delta x - \\frac{\\alpha}{t}[\\nabla f(x + t\\Delta x) - \\nabla f(x)] || \\\\\n&= || \\Delta x - \\alpha \\nabla^2 f(x)\\Delta x || \\\\\n& \\leq q||\\Delta x|| (\\#eq:convex)\n\n\\end{align}\n\n\\@ref(eq:convex) holds by the contractivity and it means that $||I_n - \\alpha\\nabla^2 f(x)||\\leq q$. Therefore, we derive\n\n$$ \\frac{1-q}{\\alpha}I_n  \\leq \\nabla^2 f(x) \\leq \\frac{1+q}{\\alpha} I_n $$. Therefore $f(x)$ is strongly convex.\n\nSuppose $f(x)$ is strongly convex, go backward of the above proof, we assert $g(x)$ is contractive.\n```\n\n\\\n\nWe close this subsection by pointing out that the scheme discussed above are not optimal. Here the *optimal* is in terms of the lower complexity bound. In [@nesterov2013introductory], Nesterov proves the following two theorems. \n\n**Assumption LC**  An interative method generates a sequence of test points $\\{x^k\\}$ such that $x^k \\in x^0 + \\sum_{i=1}^{k-1} a^i\\nabla f(x^{i})$.\n\n```{theorem, label=\"slow\"}\nFor any $k$, $1 \\leq k \\leq \\frac{1}{2} (n-1)$, and any $x_0\\in R^n$, there exists a continuous differentiable and gradient Lipschitz continuous convex function $f(x)$ such that for any first order method satisfying the Assumption LC, we have \n\n$$ f(x_k) - f^* \\geq \\frac{3L||x^0 - x^*||^2}{32(k+1)^2}$$\n$$ ||x^k - x^*|| \\geq \\frac{1}{32} || x^0 - x^* ||^2,$$\n\nwhere $L$ is the gradient Lipschitz continuous parameter.\n```\n\n```{theorem, label=\"llow\"}\nFor any $x_0\\in R^n$ and constant $\\ell>0,L>1$, there exists a continuous differentiable strongly convex function $f(x)$ such that for any first order method satisfying Assumption LC, we have \n\n$$ f(x_k) - f^* \\geq \\frac{\\ell}{2}(\\frac{\\sqrt{L/\\ell}-1}{\\sqrt{L/\\ell}+1})^{2k}|| x^0 - x^* ||^2$$\n$$ ||x^k - x^*||^2 \\geq (\\frac{\\sqrt{L/\\ell}-1}{\\sqrt{L/\\ell}+1})^{2k} || x^0 - x^* ||^2,$$\n\nwhere $L$ and $\\ell$ are the gradient Lipschitz continuous parameter and strongly convex parameter.\n```\n\\\nTheorem \\@ref(thm:slow) and Theorem \\@ref(thm:llow) leaves room for our next section's topic - acceleartion.\n\n## Acceleartion\n\n### Motivation for acceleration.\n\nApart from what we mentioned above the lower complexity bound, we note that even we can obtain linear rate of convergence, gradient descent method can still be dreadfully slow in practice. Let's look at the example below.\n\n```{example, label=\"plaingd\"}\nIf $f(x) = \\frac{1}{2} x^TAx$, where $A = \\begin{pmatrix}200 & 0 \\\\ 0 & 1 \\end{pmatrix}$. And we run gradient descent method with starting point $x^0 = (0.1,0.1)$ and step-size $\\alpha = \\frac{2}{201}$. If we run gradient descent method 100 times, we will get following results.\n```\n\n```{r contour_g,echo=FALSE}\ngd <- function(x0,lipschtiz,strongconvex,iterations){\n  A = cbind(c(lipschtiz,0),c(0,strongconvex))\n  x_seq = matrix(ncol=(iterations+1), nrow=2)\n  fval_seq = rep(0,iterations+1)\n  x_seq[,1] = x0\n  fval_seq[1] = 0.5 * ( t(x0) %*% A %*% x0)\n  for (i in seq_len(iterations)){\n    x_seq[,i+1] = x_seq[,i] - (2 / (lipschtiz + strongconvex)) * ( A %*%  x_seq[,i])\n    fval_seq[i+1] =  0.5 * ( t(x_seq[,i+1]) %*% A %*% x_seq[,i+1])\n  }\n  # for contour plot\n  x1 <- seq(-(1.1*x0[1,]),(1.1*x0[2,]),length.out = 100)\n  x2 <- seq(-(1.1*x0[1,]),(1.1*x0[2,]),length.out = 100)\n  z <- matrix(ncol=100,nrow=100)\n  for (i in seq_len(100)){\n    for (j in seq_len(100)){\n      x <- c(x1[i],x2[j])\n      z[i,j] <- 0.5 * t(x) %*% A %*% x\n    }\n  }\n  results = list (fval_seq = fval_seq, x_seq = x_seq, x1=x1, x2=x2, z=z)\n  return (results)\n}\nx0 = matrix(c(0.1,0.1),nrow=2)\nresults = gd(x0,lipschtiz=200,strongconvex=1,iterations=100)\n# par(mfrow = c(1,2))\ncontour(x = results$x1, y = results$x2,z = results$z, \n        nlev = 50, lty = 2, method = \"simple\",\n        drawlabels = F,axes = TRUE,\n        xlab=expression(x[1]),ylab=expression(x[2]))\nlines(x=results$x_seq[1,],y=results$x_seq[2,],col=\"blue\",pch=1)\npoints(x=0,y=0,col=\"red\")\n```\n\nFigure above shows that the path of iterates is zig-zag, hence converging slowly to the global optimal solution $(0,0)$. The reason lies in the condition number $\\kappa = L/\\ell$, which is too large. And in\n$$f(x_k) - f(x^*) \\leq \\frac{L}{2}\\big(\\frac{L-\\ell}{L+\\ell}\\big)^{2k}||x^0 - x^*||^2.$$\nthe shrinkage factor $\\frac{L-\\ell}{L+\\ell}$ is close to $1$, which means, you can gain little improvement in each iteration.\n\nIntuitively, to get around this situation, we have to avoid \"oscillation\" phenomenon. One commonly used strategy is to the previous stage's direction $x^{k-1}-x^k$ to modify the current direction $\\nabla f(x^k)$. We go along the composite direction $\\xi^k (x^{k-1}-x^k)  + \\zeta^k\\nabla f(x^{k})$ to find the $x^{k+1}$.\n\n### Why we can accelerate? \n\nTake the folowing example to illustrate.\n\n```{example}\nConsider using gradient descent methods to solve the problem shown below.\n\n$$\\mathop{\\text{agrmin}}_{x\\in R^n} \\frac{1}{2}x^TAx - b^Tx$$\n\nwhere $\\ell I_n \\leq A \\leq L I_n$. All that gradient descent does is to compute the sequence of points \n\n$$ x^{k+1} = x^k - t^k \\nabla f(x^k) $$.\n\nLet's set $t^k = t= 2/(\\ell + L)$ and run gradient descent with starting points $x^0 = tb$. We can then check that\n\n$$ x^k = \\Big(\\sum_{j=0}^k (I-A')^k\\Big)b' $$ \n\nwhere $A′=tA$ and $b′=tb$. Since $||A'||<1$, $x^k \\rightarrow x^* = A^{-1}b$. This is the generlization of following result\n$$\\frac{1}{x} = \\sum_{k=0}^\\infty (1-x)^k, \\forall |x|<1 .$$\n\nMoreover, the approximation error when truncating the series at degree $k$ is $O( (1-x)^k)).$\n\nSo to improve on gradient descent it suffices to find a better low-degree approximation to the scalar function $1/x$. Indeed, we can save a square root in the degree while achieving the same error by using [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials). Chebyshev polynomials of the first kind takes the form of\n\n$$\n\\begin{aligned}\nT_{0}(x)&=1\\\\\nT_{1}(x)&=x\\\\\nT_{n+1}(x)&=2xT_{n}(x)-T_{n-1}(x).\n\\end{aligned}\n$$\n\nThe Chebyshev polynomials satisfy a simple recursive definition that defines the k-th degree polynomial in terms of the previous two polynomials. This means that accelerated gradient descent only needs the previous two gradients with suitable coefficients:\n\n$$ x_k = x_{k-1} - \\alpha_k \\nabla f(x_{k-1}) + \\beta_k \\nabla f(x_{k-2}). $$\nThis conicides with the general form Nesterov Accelerated Gradient Descent Method(NAGD). Detailed discussion can be found [here](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html).\n```\n\n### Heavy Ball Methods v.s NAGD Method\n\nLet's put our intuition into concrete mathematical form, which gives rise to the Heavy Ball Method.\n\n\n| Name |  Heavy Ball Methods|  \n| --- | --- |\n|  Scheme| Set $x^0 \\in R^n$, and run $x^{k+1} = x^k - [\\alpha\\nabla f(x^k) + \\beta (x^{k-1} - x^k)]$  | \n\nWhat Nesterov does is mcuh more \"smart\". To be specific, instead of using current iterate's gradient information, $\\nabla f(x^k)$, Nesterov suggests using the approximate position of $x^{k+1}$, given by $x^k  + \\gamma^{k-1}(x^{k-1} - x^k)$.\n\n\n| Name |  NAGD Methods|  \n| --- | --- |\n|  Scheme| Set $x^0 \\in R^n$, and run $x^{k+1} = x^k  - \\frac{1}{L}\\nabla f[x^k  + \\gamma^{k-1}(x^{k-1} - x^k)] + \\gamma^{k-1}(x^{k-1} - x^k)$ |\n\nThe NAGD Methods can be also seen as a two stage iterative methods, by re-constructing $x^{k+1} = x^k  - \\frac{1}{L}\\nabla f[x^k  + \\gamma^{k-1}(x^{k-1} - x^k)] + \\gamma^{k-1}(x^{k-1} - x^k)$  as\n\n$$\n\\begin{cases}\n  y^{k+1} &= x^k - \\alpha f(x^k)\\; &(1)\\\\\n  x^{k+1} &= (1 - \\gamma^k)y^{k+1} + \\gamma^k y^k\\; &(2)\n\\end{cases}\n$$\n\n* (1) performs a simple step of gradient descent to go from $x^k$ to $y^{k+1}$ and then,\n* (2) slides a little bit further than $y^{k+1}$ in the direction given by the previous iterate $y^k$.\n\n**Remark: Parameter Selection.** \nThe $\\alpha,\\beta,\\gamma^k$ is given as follows to guarantee convergence.\n\n$$\n\\begin{align}\n& \\alpha = \\frac{4}{L}\\frac{1}{(1 + \\frac{1}{\\sqrt{L/\\ell}})^2} \\\\\n& \\beta = (1 - \\frac{2}{\\sqrt{L/\\ell}+1})^2 \\\\\n& \\gamma^k = \\frac{1-\\lambda^k}{\\lambda^k + 1}\n\\end{align}\n$$\nwhere $\\lambda^0=0, \\lambda^k = \\frac{1+ \\sqrt{1 + 4(\\lambda^{k-1})^2}}{2}$.\n\nWe conclude this section by performing two methods on the problem of Example\\@ref(ex:plaingd).\n\n### When does the NAGD Methods fail?\n",
    "created" : 1495029635579.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3010431504",
    "id" : "333C01D1",
    "lastKnownWriteTime" : 1495129202,
    "last_content_update" : 1495129202118,
    "path" : "~/Desktop/GDreview/01-Gradient_Methods.Rmd",
    "project_path" : "01-Gradient_Methods.Rmd",
    "properties" : {
        "docOutlineVisible" : "1",
        "last_setup_crc32" : "6EF20EEfc3eb810"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
[
["01-Gradient_Methods.html", "Chapter 1 Gradient Decent Methods 1.1 Problem with additional assumptions 1.2 Convex situations 1.3 Acceleartion", " Chapter 1 Gradient Decent Methods Problem Formulation \\(\\min_{x\\in R^n} f(x)\\) Function class Continuously Differentiable and bouded below \\(R^n\\) Goal Find local minimum \\(x^{*}\\) Scheme Choose \\(x^0\\in R^n\\), and run \\(x^{k+1} = x^k - \\alpha^k\\nabla f(x^k)\\) 1.1 Problem with additional assumptions In many literatures, problems of interest are naturally endowed with some basic assumptions so that beautiful analytical results can be derived, such as the convergence rate. One might ask that why these assumptioms are important and what are intuitions behind these assumtions? We try to answer these questions in this section. 1.1.1 Lipschtiz constant Problems of interest are naturally endowed with the Lipschitz continous gradient property, that is Definition 1.1 We can \\(f(x)\\) is gradient Lipschitz continous if \\[||\\nabla f(x) - \\nabla f(y) || \\leq L || x - y|| \\forall x,y\\in R^n.\\] Here \\(||.||\\) means the Euclidean norm. Remark Intuitively, the Lipschitz continous gradient property enforces \\(f(x)\\) not to be “too sharp”. For example, the \\(f(x) = |x|\\) dosen’t enjoy this property. Remark If \\(f(x)\\) is gradient Lipschitz continous, then it is upper bounded by a quadratic function \\(\\phi_1(x)\\), that is \\[ f(x) \\leq f(x_0) + \\nabla f(x_0)^T(x-x_0) + \\frac{L}{2} ||x-x_0||^2 := \\phi_1(x) \\quad \\forall x,x_0\\in R^n \\] also \\(f(x)\\) is the upper bound of another quadratic function \\(\\phi_2(x)\\), taking form of \\[ \\phi_2(x) := f(x^0) + \\nabla f(x_0)^T(x-x_0) - \\frac{L}{2} ||x-x_0||^2. \\] See Figure1.1 for example. Figure 1.1: An illustration of the gradient Lipschtiz continuous propety. To verify this remark, we can apply the following lemma. Lemma 1.1 If \\(f(x)\\) is continuously differentiable and gradient Lipschtiz continuous, then for any \\(x,y\\) the following holds: \\[ | f(x) - f(y) - (y-x)^T\\nabla f(x)| \\leq \\frac{L}{2} || y-x ||^2\\] Corollary 1.1 From Lemma1.1, we assert that if \\(f(x)\\) is continuously differentiable and gradient Lipschtiz continuous then the Hessian of \\(f(x)\\) is bounded by \\(LI_n\\), where \\(I_n\\) is \\(n\\times n\\) unit matrix. A easy way to check the gradient Lipschitz continous assumption is to apply the Lemma1.2(Nesterov 2013) Lemma 1.2 If \\(f(x)\\) is twice differentiable and satisfies \\(||\\nabla f(x) - \\nabla f(y) || \\leq L || x - y||\\) if and only if \\[ ||\\nabla^2 f(x) || \\leq L, \\forall x\\in R^n. \\] For matrix case, \\(||A|| = \\sqrt{\\lambda_{max}(A^TA)},\\forall A\\in R^{n\\times n}.\\) 1.1.2 Convexity If \\(f(x)\\) is convex, we enjoy the following nice property: The local minimum we find is also the global minimum; Using the First order optimality condition, we can dramatically simplify our problem. That is, we only have to construct a sequence \\(\\{x^k\\}\\) that satifies \\(\\lim_{k\\rightarrow +\\infty}||\\nabla f(x^k)||=0\\) 1.1.3 Strong Convexity We now only present a geometric interpretation of strong convexity and will go back to this point later when talking about the rate of convergence. Definition 1.2 We can \\(f(x)\\) is strongly convex if \\[ f(x) \\geq f(x_0) + \\nabla f(x_0)^T(x-x_0) + \\frac{\\ell}{2} ||x-x_0||^2 \\forall x,x_0\\in R^n \\] This means that we can squeeze a parabola between the tangent plane at \\(x_0\\) given by the gradient and the function itself. Thus, the strong convexity assumption enforce the \\(f(x)\\) not to be too “flat”. Figure 1.2: An illustration of the strongly convex propety. 1.2 Convex situations Without additional assumptions, we can not guarantee convergence even to a local minimum and estimate the global performance of the proposed schemes. In general, the gradient method convergece only to a stationary point of \\(f(x)\\). Therefore, from now on, we only focus on convex cases. 1.2.1 step-size selection rule and the rate of convergence The step-size \\(\\alpha^k\\) plays a key role in the convergence of the gradient descent method. If \\(\\alpha^k\\) is too small, more iterations will be required to achieve the \\(\\epsilon\\)-accuracy (\\(|f(x^k)-f(x^*)|\\leq \\epsilon\\)). If \\(\\alpha^k\\) is too large, the scheme fails to converge. We here present two theoretical optimal step-size selection rules. If convex \\(f(x)\\) is gradient Lipschtiz continuous, then the optimal \\(\\alpha^k = \\frac{1}{L}\\). If strongly convex \\(f(x)\\) is gradient Lipschtiz continuous, then the optimal \\(\\alpha^k = \\frac{2}{L+\\ell}\\). Here, optimal means \\[ \\alpha^k = \\mathop{\\text{argmin}}_{\\alpha} f(x^k - \\alpha\\nabla f(x^k)). \\] It has been proved that For convex and gradient Lipschtiz continuous \\(f(x)\\), with chosen step-size \\(\\alpha^k = \\frac{1}{L}\\) , the rate of convergence is sub-linear. To be specific, \\[f(x_k) - f(x^*) \\leq \\frac{2L||x^0 - x^*||^2}{k+4}.\\] For strongly convex and gradient Lipschtiz continuous \\(f(x)\\), with chosen step-size \\(\\alpha^k = \\frac{2}{L + \\ell}\\) , the rate of convergence is linear. To be specific, \\[f(x_k) - f(x^*) \\leq \\frac{L}{2}\\big(\\frac{L-\\ell}{L+\\ell}\\big)^{2k}||x^0 - x^*||^2.\\] Technically, the above results are mainly derived from two steps: Find how much you can improve in a single interation by establishing \\[f(x^{k+1}) \\leq f(x^k) - \\omega_1 ||\\nabla f(x_k)|| \\] Find how far at most we still need to go to recach the minimum by establishing \\[f(x^k) - f(x^*) \\leq \\omega_2||\\nabla f(x_k)||\\] 1.2.2 Why linear rate of convergence? Before we answer this question, we first list the definition of a contraction mapping and the Banach fixed-point theorem. Definition 1.3 (Contraction Mapping) Let \\((X, d)\\) be a metric space. Then a map \\(T : X \\rightarrow X\\) is called a contraction mapping on \\(X\\) if there exists \\(q \\in [0, 1)\\) such that \\[ d(T(x),T(y))\\leq qd(x,y) \\forall x, y \\in X.\\] Theorem 1.1 (Banach Fixed Point Theorem) Let \\((X, d)\\) be a non-empty complete metric space with a contraction mapping \\(T : X \\rightarrow X\\). Then \\(T\\) admits a unique fixed-point \\(x^*\\) in \\(X\\) (i.e. \\(T(x^*) = x^*\\)). Furthermore, \\(x^*\\) can be found as follows: start with an arbitrary element \\(x_0\\) in \\(X\\) and define a sequence \\(\\{x_n\\}\\) by \\(x_n = T(x_{n−1})\\), then \\(x_n \\rightarrow x^*\\). Moreover, \\[ d(x^{*},x_{n+1})\\leq qd(x^{*},x_{n}) \\] Then we establish connections between contraction mapping and linear rate of convergence through the gradient decent method, which takes the form of \\[x^{k+1} = x^k - \\alpha\\nabla f(x^k),\\] If we define a mapping as \\(g(x) = x - \\alpha\\nabla f(x)\\) and suppose \\(g(x)\\) is contractive on \\(R^n\\). Assume there exists a stationary point \\(x^*\\), then starting from any point \\(x^0\\), we derive \\[ \\begin{align} ||x^{k+1} - x^*|| &amp;= ||x^{k} - \\alpha\\nabla f(x^k) - x^*|| \\\\ &amp;= || g(x^k) - g(x^*)|| \\\\ &amp; \\leq q||x^k - x^*|| \\\\ &amp; \\leq q^{k+1}|| x^0 - x^* || \\tag{1.1} \\end{align} \\] (1.1) indicates that the rate of convergence is linear. Finally, we assert that Lemma 1.3 If \\(f(x)\\) is twice continuously differentiable, the \\(g(x)\\) is a contraction mapping if and only if \\(f(x)\\) is strongly convex on \\(R^n\\). Proof Suppose \\(g(x)\\) is contractivie, then we derive, \\[\\begin{align} \\lim_{t\\rightarrow 0^+} \\frac{1}{t} || g(x + t\\Delta x) - g(x) || &amp;= \\lim_{t\\rightarrow 0^+} || \\Delta x - \\frac{\\alpha}{t}[\\nabla f(x + t\\Delta x) - \\nabla f(x)] || \\\\ &amp;= || \\Delta x - \\alpha \\nabla^2 f(x)\\Delta x || \\\\ &amp; \\leq q||\\Delta x|| \\tag{1.2} \\end{align}\\] (1.2) holds by the contractivity and it means that \\(||I_n - \\alpha\\nabla^2 f(x)||\\leq q\\). Therefore, we derive \\[ \\frac{1-q}{\\alpha}I_n \\leq \\nabla^2 f(x) \\leq \\frac{1+q}{\\alpha} I_n \\]. Therefore \\(f(x)\\) is strongly convex. Suppose \\(f(x)\\) is strongly convex, go backward of the above proof, we assert \\(g(x)\\) is contractive. We close this subsection by pointing out that the scheme discussed above are not optimal. Here the optimal is in terms of the lower complexity bound. In (Nesterov 2013), Nesterov proves the following two theorems. Assumption LC An interative method generates a sequence of test points \\(\\{x^k\\}\\) such that \\(x^k \\in x^0 + \\sum_{i=1}^{k-1} a^i\\nabla f(x^{i})\\). Theorem 1.2 For any \\(k\\), \\(1 \\leq k \\leq \\frac{1}{2} (n-1)\\), and any \\(x_0\\in R^n\\), there exists a continuous differentiable and gradient Lipschitz continuous convex function \\(f(x)\\) such that for any first order method satisfying the Assumption LC, we have \\[ f(x_k) - f^* \\geq \\frac{3L||x^0 - x^*||^2}{32(k+1)^2}\\] \\[ ||x^k - x^*|| \\geq \\frac{1}{32} || x^0 - x^* ||^2,\\] where \\(L\\) is the gradient Lipschitz continuous parameter. Theorem 1.3 For any \\(x_0\\in R^n\\) and constant \\(\\ell&gt;0,L&gt;1\\), there exists a continuous differentiable strongly convex function \\(f(x)\\) such that for any first order method satisfying Assumption LC, we have \\[ f(x_k) - f^* \\geq \\frac{\\ell}{2}(\\frac{\\sqrt{L/\\ell}-1}{\\sqrt{L/\\ell}+1})^{2k}|| x^0 - x^* ||^2\\] \\[ ||x^k - x^*||^2 \\geq (\\frac{\\sqrt{L/\\ell}-1}{\\sqrt{L/\\ell}+1})^{2k} || x^0 - x^* ||^2,\\] where \\(L\\) and \\(\\ell\\) are the gradient Lipschitz continuous parameter and strongly convex parameter. Theorem 1.2 and Theorem 1.3 leaves room for our next section’s topic - acceleartion. 1.3 Acceleartion 1.3.1 Motivation for acceleration. Apart from what we mentioned above the lower complexity bound, we note that even we can obtain linear rate of convergence, gradient descent method can still be dreadfully slow in practice. Let’s look at the example below. Example 1.1 If \\(f(x) = \\frac{1}{2} x^TAx\\), where \\(A = \\begin{pmatrix}200 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\). And we run gradient descent method with starting point \\(x^0 = (0.1,0.1)\\) and step-size \\(\\alpha = \\frac{2}{201}\\). If we run gradient descent method 100 times, we will get following results. Figure above shows that the path of iterates is zig-zag, hence converging slowly to the global optimal solution \\((0,0)\\). The reason lies in the condition number \\(\\kappa = L/\\ell\\), which is too large. And in \\[f(x_k) - f(x^*) \\leq \\frac{L}{2}\\big(\\frac{L-\\ell}{L+\\ell}\\big)^{2k}||x^0 - x^*||^2.\\] the shrinkage factor \\(\\frac{L-\\ell}{L+\\ell}\\) is close to \\(1\\), which means, you can gain little improvement in each iteration. Intuitively, to get around this situation, we have to avoid “oscillation” phenomenon. One commonly used strategy is to the previous stage’s direction \\(x^{k-1}-x^k\\) to modify the current direction \\(\\nabla f(x^k)\\). We go along the composite direction \\(\\xi^k (x^{k-1}-x^k) + \\zeta^k\\nabla f(x^{k})\\) to find the \\(x^{k+1}\\). 1.3.2 Why we can accelerate? Take the folowing example to illustrate. Example 1.2 Consider using gradient descent methods to solve the problem shown below. \\[\\mathop{\\text{agrmin}}_{x\\in R^n} \\frac{1}{2}x^TAx - b^Tx\\] where \\(\\ell I_n \\leq A \\leq L I_n\\). All that gradient descent does is to compute the sequence of points \\[ x^{k+1} = x^k - t^k \\nabla f(x^k) \\]. Let’s set \\(t^k = t= 2/(\\ell + L)\\) and run gradient descent with starting points \\(x^0 = tb\\). We can then check that \\[ x^k = \\Big(\\sum_{j=0}^k (I-A&#39;)^k\\Big)b&#39; \\] where \\(A′=tA\\) and \\(b′=tb\\). Since \\(||A&#39;||&lt;1\\), \\(x^k \\rightarrow x^* = A^{-1}b\\). This is the generlization of following result \\[\\frac{1}{x} = \\sum_{k=0}^\\infty (1-x)^k, \\forall |x|&lt;1 .\\] Moreover, the approximation error when truncating the series at degree \\(k\\) is \\(O( (1-x)^k)).\\) So to improve on gradient descent it suffices to find a better low-degree approximation to the scalar function \\(1/x\\). Indeed, we can save a square root in the degree while achieving the same error by using Chebyshev polynomials. Chebyshev polynomials of the first kind takes the form of \\[ \\begin{aligned} T_{0}(x)&amp;=1\\\\ T_{1}(x)&amp;=x\\\\ T_{n+1}(x)&amp;=2xT_{n}(x)-T_{n-1}(x). \\end{aligned} \\] The Chebyshev polynomials satisfy a simple recursive definition that defines the k-th degree polynomial in terms of the previous two polynomials. This means that accelerated gradient descent only needs the previous two gradients with suitable coefficients: \\[ x_k = x_{k-1} - \\alpha_k \\nabla f(x_{k-1}) + \\beta_k \\nabla f(x_{k-2}). \\] This conicides with the general form Nesterov Accelerated Gradient Descent Method(NAGD). Detailed discussion can be found here. 1.3.3 Heavy Ball Methods v.s NAGD Method Let’s put our intuition into concrete mathematical form, which gives rise to the Heavy Ball Method. Name Heavy Ball Methods Scheme Set \\(x^0 \\in R^n\\), and run \\(x^{k+1} = x^k - [\\alpha\\nabla f(x^k) + \\beta (x^{k-1} - x^k)]\\) What Nesterov does is mcuh more “smart”. To be specific, instead of using current iterate’s gradient information, \\(\\nabla f(x^k)\\), Nesterov suggests using the approximate position of \\(x^{k+1}\\), given by \\(x^k + \\gamma^{k-1}(x^{k-1} - x^k)\\). Name NAGD Methods Scheme Set \\(x^0 \\in R^n\\), and run \\(x^{k+1} = x^k - \\frac{1}{L}\\nabla f[x^k + \\gamma^{k-1}(x^{k-1} - x^k)] + \\gamma^{k-1}(x^{k-1} - x^k)\\) The NAGD Methods can be also seen as a two stage iterative methods, by re-constructing \\(x^{k+1} = x^k - \\frac{1}{L}\\nabla f[x^k + \\gamma^{k-1}(x^{k-1} - x^k)] + \\gamma^{k-1}(x^{k-1} - x^k)\\) as \\[ \\begin{cases} y^{k+1} &amp;= x^k - \\alpha f(x^k)\\; &amp;(1)\\\\ x^{k+1} &amp;= (1 - \\gamma^k)y^{k+1} + \\gamma^k y^k\\; &amp;(2) \\end{cases} \\] performs a simple step of gradient descent to go from \\(x^k\\) to \\(y^{k+1}\\) and then, slides a little bit further than \\(y^{k+1}\\) in the direction given by the previous iterate \\(y^k\\). Remark: Parameter Selection. The \\(\\alpha,\\beta,\\gamma^k\\) is given as follows to guarantee convergence. \\[ \\begin{align} &amp; \\alpha = \\frac{4}{L}\\frac{1}{(1 + \\frac{1}{\\sqrt{L/\\ell}})^2} \\\\ &amp; \\beta = (1 - \\frac{2}{\\sqrt{L/\\ell}+1})^2 \\\\ &amp; \\gamma^k = \\frac{1-\\lambda^k}{\\lambda^k + 1} \\end{align} \\] where \\(\\lambda^0=0, \\lambda^k = \\frac{1+ \\sqrt{1 + 4(\\lambda^{k-1})^2}}{2}\\). We conclude this section by performing two methods on the problem of Example1.1. 1.3.4 When does the NAGD Methods fail? Test "]
]
